{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Yiou Huang: conceptualization, definition\n",
    "- Johnson Chung:  data description\n",
    "- Daniel Galicia Ortiz: data relevance\n",
    "- Wenxin Miao: data wrangling\n",
    "- Jordan Chang: data wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To what extent does (the user’s attitude towards AI: another individual VS. a tool) the use of anthropomorphic versus instrumental language in app store reviews predict user satisfaction with popular AI models (including ChatGPT, Gemini, and Claude), as measured by star ratings? \n",
    "\n",
    "In this study, we operationalize anthropomorphic language as textual expressions that frame an AI system as possessing human-like qualities—including mental states (e.g., thinking, understanding), emotions (e.g., frustration, excitement), social abilities (e.g., listening, communicating), agency (e.g., deciding, choosing), or personhood (e.g., identity, vulnerability)—thereby treating the AI as another individual rather than as a functional tool (Source 1). Conversely, instrumental language is defined as language that frames the AI system in purely functional or technical terms, emphasizing its utility, performance characteristics, and machine-like properties without attributing human characteristics (e.g., the algorithm processes requests efficiently, the software generates accurate outputs). User attitudes toward AI—whether they perceive it as another individual or as a tool—are expressed through these linguistic choices: when users anthropomorphize, their language reveals an implicit mental model of the AI as a social actor deserving of human-like treatment, whereas instrumental language reflects a mental model of the AI as an object to be evaluated solely on functional performance (Source 2; Source 3).\n",
    "\n",
    "We employ a two-stage measurement approach that combines automated computational scoring with manual validation to ensure both scalability and accuracy. In the first stage, we apply AnthroScore (Source 4), a computational linguistic measure that quantifies the degree to which a non-human entity is implicitly framed as human-like by the surrounding linguistic context. AnthroScore uses a masked language model (RoBERTa) to compute, for each mention of the AI system in a review (e.g., ChatGPT, the app), the probability that the masked entity would be human versus non-human based on contextual cues; the resulting continuous score indicates whether the entity is framed anthropomorphically (positive scores), instrumentally (negative scores), or neutrally (scores near zero) (Source 4). This automated approach allows us to process our full dataset of app store reviews efficiently while providing a validated, lexicon-free metric that captures implicit anthropomorphism beyond simple keyword matching.\n",
    "\n",
    "In the second stage, we validate and refine our classifications through manual coding using the taxonomy developed by DeVrio et al. (2025), which identifies 19 distinct types of linguistic expressions that contribute to anthropomorphism of language technologies. This taxonomy, derived from empirical analysis of user interactions with AI systems and organized across five conceptual dimensions—cognitive abilities (e.g., understands, learns), social abilities (e.g., listens, communicates), feelings and desires (e.g., gets frustrated, wants to help), physical actions and embodiment (e.g., looked at, grabbed), and identity and personhood (e.g., my friend, has a personality)—provides a granular framework for identifying specific anthropomorphic expressions that may appear in app reviews (Source 1). We randomly sample 10% of reviews stratified by AnthroScore levels (low, medium, high) and manually code them for the presence of anthropomorphic expression types, allowing us to verify that computationally-flagged reviews do indeed contain anthropomorphic language and to identify any systematic patterns the automated scoring may miss.\n",
    "\n",
    "Based on the combined evidence from AnthroScore and manual coding, we classify each review into one of three categories: anthropomorphic (high AnthroScore and/or presence of two or more expression types from the DeVrio et al. taxonomy), instrumental (low or negative AnthroScore, absence of anthropomorphic expressions, and presence of technical/functional language), or neutral/mixed (ambiguous cases, which we exclude from the primary analysis). This triangulated approach provides robust construct validity by leveraging the scalability of computational methods alongside the nuance and verification that manual coding affords, ensuring that our independent variable accurately captures the distinction between treating AI as an individual versus treating it as a tool.\n",
    "\n",
    "**Reference**\n",
    "1. DeVrio, A., Cheng, M., Egede, L., Olteanu, A., & Blodgett, S. L. (2025). A taxonomy of linguistic expressions that contribute to anthropomorphism of language technologies. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25). https://doi.org/10.1145/3706598.3714038   \n",
    "2. Epley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886. https://doi.org/10.1037/0033-295X.114.4.864   \n",
    "3. Nass, C., Steuer, J., & Tauber, E. R. (1994). Computers are social actors. In Proceedings of the CHI '94 Conference on Human Factors in Computing Systems (pp. 418–423). https://doi.org/10.1145/191666.191703   \n",
    "4. Cheng, M., Gligorić, K., Piccardi, T., & Jurafsky, D. (2024). AnthroScore: A computational linguistic measure of anthropomorphism. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), Vol. 1: Long Papers (pp. 807–825). https://doi.org/10.18653/v1/2024.eacl-long.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that AI has a profound impact on our lives. Before the 2010s, AI largely belonged to the realm of fiction, imagined as computers capable of humanlike conversation, reasoning and sometimes human emotion. In recent years, however, this fiction has become reality with the development of advanced models such as ChatGPT and Gemini, with roughly about 41% of Americans saying that they use these or similar tools. (Source 6) These large language models (LLMs) make AI interactions feel more human, enabling deeper integration into daily life. At the same time, a growing concern about AI has slowly grown over the last year, with topics ranging from jobs and privacy, to the decline of social interaction. (Source 5) Together, these examples highlight the wide range of human attitudes toward AI and raise an important question: can we predict how people perceive an AI tool based on how they treat and interact with it? This is a question that we feel the need to find some answer to.\n",
    "\n",
    "As noted earlier, AI is no longer a niche technology. It has become increasingly normalized in society, particularly among younger generations. For example, roughly two-thirds of teenagers in the United States report having used an AI chatbot (Source 7). This raises a natural follow-up question: how do people tend to perceive AI? The answer is somewhat surprising. It is becoming more common for individuals to develop personal or relational attitudes toward AI chatbots. One study found that 38% of users believe that large language models will eventually form “deep relationships with humans” (Source 8). Other research has also suggested a growing association between AI and emotionally meaningful human interaction. Anthropomorphism, attributing human characteristics to AI, has been shown to increase users’ willingness to adopt AI services (Source 9). Although that study emphasized the need for further testing, it still identified a positive relationship between anthropomorphism and user engagement. Together, these findings help lay the groundwork for the question we aim to investigate.\n",
    "\n",
    "Unfortunately no study is without limitations, and by examining prior research and studies we can identify weaknesses in data collection and methodology. For instance, some studies rely heavily on participants’ beliefs and self-reports, which can be influenced by popular culture and/or media narratives, limited technical understanding, and even the wording of survey questions. Framing effects—such as using terms like “erosion,” “loss,” or “control”—can shape how respondents interpret and answer questions (Source 5). Other studies face issues related to human estimation and omission. When research depends on approximations, results may overestimate or underestimate reality. Additionally, some studies do not distinguish how AI is used across contexts, such as for work, personal, or emotional purposes (Source 7). Although Source 7 draws on a nationally representative, demographically weighted sample, this does not fully address contextual gaps in AI usage. Because our focus is on measuring anthropomorphic language, no single dataset will be perfect; however, combining multiple datasets can help mitigate these limitations and produce more balanced insights.\n",
    "\n",
    "While the research and articles above analyze different aspects of the issue—such as public views of AI or patterns of AI usage—we believe there is a need for a complementary approach to determine whether a relationship between our two factors truly exists. To properly identify such a relationship, it is important to consider multiple external variables to ensure that any observed connection is meaningful rather than coincidental. In collecting data, particularly from app reviews, we plan to screen for anthropomorphic language used to describe or address AI systems. We also propose that several contextual factors may influence this language. These include AI usage (for example, whether AI is used for work, advice, or emotional support), AI purpose (since different systems are designed for general assistance versus specialized chatbot functions), and AI customization (such as response style, tone, or added voice features, including perceived gender). By scraping for different sets of key words and language we can infer if each AI is used for work, advice, emotional support. Although with our current datasets, we don’t have the information for purpose and customization but both can be obtained and put in a dataset with some research. Each of these elements may shape how users conceptualize and describe AI. By integrating these dimensions, by relating the level of anthropomorphic language (more of that in the data section), the Ai’s purpose, usage, and level of personalization and seeing if a pattern emerges with the star ratings, we aim to better understand whether there is a relationship between how people perceive AI and the extent to which they talk about it in human-like versus machine-like terms(he/she/them vs it).\n",
    "\n",
    "**References**\n",
    "5. https://www.pewresearch.org/science/2025/09/17/how-americans-view-ai-and-its-impact-on-people-and-society/  \n",
    "6. https://techequity.us/2025/10/07/how-people-really-feel-about-ai-from-sea-to-shining-se/   \n",
    "7. https://www.pewresearch.org/wp-content/uploads/sites/20/2025/12/PI_2025.12.09_Teens-Social-Media-AI_REPORT.pdf  \n",
    "8. https://imaginingthedigitalfuture.org/reports-and-publications/close-encounters-of-the-ai-kind/\n",
    "9. https://link.springer.com/article/10.1007/s11747-020-00762-y#Sec12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H₀): There is no significant association between the degree of anthropomorphic language used in app store reviews of AI systems and the star ratings given by users. In other words, whether a reviewer frames an AI system as a human-like entity or as a purely functional tool does not predict their reported level of satisfaction.\n",
    "\n",
    "Alternative Hypothesis (H₁): There is a significant positive association between the degree of anthropomorphic language used in app store reviews of AI systems and the star ratings given by users. Specifically, reviews that frame AI systems using more anthropomorphic language (i.e., treating AI as another individual) will be associated with higher star ratings compared to reviews that use predominantly instrumental language (i.e., treating AI as a tool).\n",
    "\n",
    "We predict a significant positive association between anthropomorphic language and higher star ratings for three converging reasons drawn from the background literature. First, Blut et al. (2021) (Source 10) synthesized 108 studies involving over 11,000 individuals and found that customer anthropomorphism of AI systems, including chatbots, functions as a positive mediator toward user intention and satisfaction, operating through increased perceptions of social presence and trust. This suggests that users who psychologically engage with AI as a social partner rather than a mechanical instrument are likely to report more favorable experiences. Second, Cheng et al. (2025) (Source 11) demonstrated using over 12,000 nationally representative responses that perceptions of AI's human-likeness significantly predict both trust and willingness to adopt AI (r² = 0.21, p < 0.001), and that these anthropomorphic perceptions increased by 34% over a single year, indicating that the trend toward viewing AI as more human-like is becoming more and more prevalent. Third, Epley, Waytz, and Cacioppo (2007) (Source 12) established that anthropomorphism is driven in part by sociality motivation: a deep human need to connect socially, which implies that users who express anthropomorphic language about an AI app are likely doing so because the interaction fulfilled a social need, an experience that would naturally translate into greater satisfaction and, consequently, higher star ratings. Furthermore, we will explore the relationships between multiple dimensions (AI usage, AI purpose, and AI customization) and people’s attitude towards AI as well as their satisfaction level.\n",
    "\n",
    "Although the weight of evidence supports our directional prediction, it is important to acknowledge that the relationship between anthropomorphic language and satisfaction may not be significant. Crolic et al. (2022) (Source 13) found across five studies, including a large real-world telecommunications dataset, that chatbot anthropomorphism actually produced a negative effect on customer satisfaction when users entered the interaction in an angry emotional state, because anthropomorphic framing inflated expectations of the chatbot's capabilities and set the stage for expectancy violations when those expectations went unmet, a dynamic that could similarly contaminate app store reviews, where frustrated users are disproportionately likely to write reviews in the first place. Furthermore, Blut et al. (2021) (Source 13) noted in their meta-analysis that the positive effects of anthropomorphism on satisfaction were heavily moderated by context and task type, meaning that in utilitarian, task-completion-oriented interactions, arguably the dominant use case for most AI apps, instrumental framing may matter as much or more than anthropomorphic framing for predicting user satisfaction.\n",
    "\n",
    "**References**\n",
    "10. Blut, M., Wang, C., Wünderlich, N. V., & Brock, C. (2021). Understanding anthropomorphism in service provision: A meta-analysis of physical robots, chatbots, and other AI. Journal of the Academy of Marketing Science, 49(4), 632–658. https://doi.org/10.1007/s11747-020-00762-y  \n",
    "11. Cheng, M., Lee, A. Y., Rapuano, K., Niederhoffer, K., Liebscher, A., & Hancock, J. T. (2025). From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors. In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT '25). https://arxiv.org/abs/2501.18045   \n",
    "12. Epley, N., Waytz, A., & Cacioppo, J. T. (2007). On seeing human: A three-factor theory of anthropomorphism. Psychological Review, 114(4), 864–886. https://doi.org/10.1037/0033-295X.114.4.864  \n",
    "13. Crolic, C., Thomaz, F., Hadi, R., & Stephen, A. T. (2022). Blame the bot: Anthropomorphism and anger in customer–chatbot interactions. Journal of Marketing, 86(1), 132–148. https://doi.org/10.1177/00222429211045687 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "- Dataset #1\n",
    "  - Dataset Name: ChatGPT Google Play Store Reviews\n",
    "  - Link to the dataset: https://github.com/COGS108/Group114_WI26/blob/master/data/00-raw/chatgpt_reviews.csv\n",
    "  - Number of observations: 5000\n",
    "  - Number of variables: 12\n",
    "- Dataset #2\n",
    "  - Dataset Name: Claude Google Play Store Reviews\n",
    "  - Link to the dataset: https://github.com/COGS108/Group114_WI26/blob/master/data/00-raw/claude_reviews.csv\n",
    "  - Number of observations: 5000\n",
    "  - Number of variables: 12\n",
    "- Dataset #3\n",
    "  - Dataset Name: ChatGPT Google Play Store Reviews\n",
    "  - Link to the dataset: https://github.com/COGS108/Group114_WI26/blob/master/data/00-raw/gemini_reviews.csv\n",
    "  - Number of observations: 5000\n",
    "  - Number of variables: 12\n",
    "\n",
    "All of the reviews are scrapped from the Google Play Store. Since the data sets are very similar other than the AI app it is scrapping, I will just condense it to one thing down here.\n",
    "\n",
    "#### Description of the variables most relevant to this project:\n",
    "\n",
    "These datasets are reviews scrapped from the Google Play Store page. The important metrics from our scrapped data are the app name, contents of the review, star rating, the amount of thumbs up. The other metrics we saw as either not important in analysis or had user information which we will exclude. The other important metric not on here yet is the anthroscore and the possible ai usage. \n",
    "\n",
    "### 1. App Name\n",
    "- **Units:** Categorical variable (text label)  \n",
    "- **Possible Values:** ChatGPT, Claude, Gemini  \n",
    "\n",
    "**Description:**  \n",
    "The app name identifies which AI application a review belongs to. This variable serves as the **common identifier** used to merge datasets and group observations by AI system.\n",
    "\n",
    "**Interpretation:**  \n",
    "Differences across app names allow comparisons between AI platforms in terms of user perception, satisfaction, and anthropomorphic framing, and personilization.\n",
    "\n",
    "**Analytical Role:**  \n",
    "Dataset identifier / grouping variable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Review Content\n",
    "- **Units:** Unstructured text data  \n",
    "- **Range:** Variable-length written user responses  \n",
    "\n",
    "**Description:**  \n",
    "The review content represents the written feedback submitted by users describing their experiences with the AI application.\n",
    "\n",
    "**Interpretation:**  \n",
    "Natural language processing methods are applied to extract meaningful patterns such as:\n",
    "- references to automation or assistance,\n",
    "- emotional or human-like descriptions,\n",
    "- perceived intelligence or agency of the AI.\n",
    "\n",
    "**Analytical Role:**  \n",
    "Primary **independent variable** used to derive additional measures such as anthropomorphic perception and possible AI usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Star Rating\n",
    "- **Units:** Integer rating scale (stars)  \n",
    "- **Range:** 1–5 stars (integer values only)  \n",
    "\n",
    "**Meaning of Values:**\n",
    "- **1 star:** Very negative experience or dissatisfaction  \n",
    "- **2 stars:** Negative evaluation  \n",
    "- **3 stars:** Neutral or mixed experience  \n",
    "- **4 stars:** Positive evaluation  \n",
    "- **5 stars:** Highly satisfied user experience  \n",
    "\n",
    "**Description:**  \n",
    "The star rating reflects the overall evaluation given by a user at the time of submitting their review.\n",
    "\n",
    "**Interpretation:**  \n",
    "Higher star ratings indicate greater perceived usefulness or satisfaction with the AI system.\n",
    "\n",
    "**Analytical Role:**  \n",
    "Primary **dependent variable**, used to measure how user satisfaction changes with anthropomorphic perception and review characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Thumbs-Up Count\n",
    "- **Units:** Count of user endorsements (number of likes)  \n",
    "- **Range:** Integer values ≥ 0  \n",
    "\n",
    "**Description:**  \n",
    "The thumbs-up count represents how many other users marked a review as helpful or agreeable.\n",
    "\n",
    "**Interpretation:**  \n",
    "Reviews with higher thumbs-up counts are assumed to reflect opinions shared by a larger portion of users and therefore hold more weight.\n",
    "\n",
    "**Typical Meaning:**\n",
    "- **Low values (0–5):** Limited visibility or agreement  \n",
    "- **Moderate values:** Some community validation  \n",
    "- **High values:** Strong agreement or highly informative review  \n",
    "\n",
    "**Analytical Role:**  \n",
    "Weighting variable used to emphasize reviews representing stronger collective sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Anthroscore\n",
    "- **Units:** Dimensionless continuous score  \n",
    "- **Range:** Real-valued (typically centered near 0)  \n",
    "\n",
    "**Description:**  \n",
    "The anthroscore quantifies the extent to which users describe the AI system using **human-like traits** versus treating it as a purely functional tool.\n",
    "\n",
    "**Interpretation of Values:**\n",
    "- **A < -1:** Strong tool-oriented framing  \n",
    "- **-1 ≤ A ≤ 1:** Neutral or mixed framing  \n",
    "- **A > 1:** Strong anthropomorphic (human-like) framing  \n",
    "\n",
    "**Probabilistic Meaning:**  \n",
    "If the anthroscore is `A`, then `e^A` represents how much more likely a review frames the AI as human rather than machine.\n",
    "\n",
    "Examples:\n",
    "- `A = 1` → `e^1 ≈ 2.7` → ~2.7× more likely human framing  \n",
    "- `A = -1` → `e^-1 ≈ 0.37` → more likely tool framing  \n",
    "\n",
    "**Analytical Role:**  \n",
    "Derived independent variable measuring perceived humanization of AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. AI_Usage\n",
    "\n",
    "- **Units:** Categorical variable (derived from review text)  \n",
    "- **Possible Values:** Workflow Usage, Emotional Usage, Informational/Tool Usage  \n",
    "\n",
    "**Description:**  \n",
    "The Type of AI Usage metric classifies how users interact with an AI system based on keywords identified within review content. Categories are assigned using word-based analysis that reflects the primary role the AI serves for the user.\n",
    "\n",
    "**Usage Categories:**\n",
    "\n",
    "- **Workflow Usage:**  \n",
    "  Indicates productivity or task-oriented interaction, such as writing, coding, brainstorming, research, or automation tasks.\n",
    "\n",
    "- **Emotional Usage:**  \n",
    "  Indicates social or emotional interaction, including conversation, comfort, support, or companionship-related language.\n",
    "\n",
    "- **Informational / Tool Usage:**  \n",
    "  Indicates factual or utility-based interaction where the AI is used primarily for obtaining information, explanations, or quick answers.\n",
    "\n",
    "**Classification Method:**  \n",
    "Reviews are categorized based on the presence of representative keywords associated with each usage type. The dominant category is assigned according to the strongest linguistic presence or dpending on the implementation have several categories assinged to it.\n",
    "\n",
    "**Analytical Role:**  \n",
    "Derived independent variable used to analyze how different AI usage patterns relate to anthropomorphic perception and user satisfaction (star rating).\n",
    "\n",
    "#### Descriptions of any shortcomings this dataset has with repsect to the project:\n",
    "The people who go out of their way to review tend to be more extremely negative or extremely positive. Because this data set takes in reviews, that means the reviews are likely polarized and not representative of the average user experience. This creates a bias where people with netrual opinions or non-strong opinions are much less likely to leave reviews. Additionally, some of the reviews are really short such as only 1 word, which is not enough context for the anthroscore to be calculated reliabily. Only having 1 word will make the anthroscore be 0, which can skew the overall results and reduce the accuracy of the analysis. Therefore, these entries will need to be cleaned before analysis. \n",
    "\n",
    "Besides the currently existing data variables, we are also missing an additional dataset that will need to be implemented later. This dataset would contain information describing the AI systems themselves, rather than user reviews. The first variable would again be the AI name, which will act as the common identifier used to merge this dataset with the review data. The next variable would be the main purpose of the AI, which categorizes what the system is primarily designed for, such as general assistance, productivity support, coding help, or conversational interaction. In addition, the dataset would include several feature-based variables, recorded as categorical indicators describing whether the AI supports capabilities such as voice interaction, gendered or human-like presentation, conversational memory, or response style customization. These features should help capture how human-like or interactive an AI system is designed to appear, independent of user perception. Overall, this additional dataset would allow us to analyze whether built-in design choices and platform features influence how users interact with, perceive, and ultimately rate different AI systems.\n",
    "\n",
    "#### How we are combining the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com', 'filename':'chatgpt_reviews.csv'},\n",
    "    { 'url': 'N/A', 'filename':'gemini_reviews.csv'},\n",
    "    { 'url': 'N/A', 'filename':'claude_reviews.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 ChatGPT Google Play Store Reviews (RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langdetect and mtranslate to detect and translate different languages\n",
    "%pip install langdetect\n",
    "%pip install mtranslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from langdetect import detect\n",
    "from mtranslate import translate\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "# load the chatgpt google play store reviews into dataset\n",
    "chat_df = pd.read_csv(\"data/00-raw/chatgpt_reviews.csv\")\n",
    "\n",
    "# drop unnecessary columns\n",
    "keep_cols = ['app_name', 'content', 'score', 'thumbsUpCount', 'at', 'appVersion']\n",
    "clean_chat_df = chat_df[keep_cols].reset_index(drop=True)\n",
    "\n",
    "# rename columns for easy reading\n",
    "clean_chat_df = clean_chat_df.rename(columns={'app_name': 'app', 'thumbsUpCount': 'like', 'at': 'date', 'appVersion': 'version'})\n",
    "\n",
    "# send file to interim folder\n",
    "file_path = 'data/01-interim/cleaned_chat.csv'\n",
    "clean_chat_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 Gemini Google Play Store reviews (RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chatgpt google play store reviews into dataset\n",
    "gemini_df = pd.read_csv(\"data/00-raw/gemini_reviews.csv\")\n",
    "\n",
    "# drop unnecessary columns\n",
    "keep_cols = ['app_name', 'content', 'score', 'thumbsUpCount', 'at', 'appVersion']\n",
    "clean_gemini_df = gemini_df[keep_cols].reset_index(drop=True)\n",
    "\n",
    "# rename columns for easy reading\n",
    "clean_gemini_df = clean_gemini_df.rename(columns={'app_name': 'app', 'thumbsUpCount': 'like', 'at': 'date', 'appVersion': 'version'})\n",
    "\n",
    "# send file to interim folder\n",
    "file_path = 'data/01-interim/cleaned_gemini.csv'\n",
    "clean_gemini_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3 Claude Google Play Store reviews (RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chatgpt google play store reviews into dataset\n",
    "claude_df = pd.read_csv(\"data/00-raw/claude_reviews.csv\")\n",
    "\n",
    "# drop unnecessary columns\n",
    "keep_cols = ['app_name', 'content', 'score', 'thumbsUpCount', 'at', 'appVersion']\n",
    "clean_claude_df = claude_df[keep_cols].reset_index(drop=True)\n",
    "\n",
    "# rename columns for easy reading\n",
    "clean_claude_df = clean_claude_df.rename(columns={'app_name': 'app', 'thumbsUpCount': 'like', 'at': 'date', 'appVersion': 'version'})\n",
    "\n",
    "# send file to interim folder\n",
    "file_path = 'data/01-interim/cleaned_claude.csv'\n",
    "clean_claude_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #4 ChatGPT-Gemini-Claude Combined Google Play Store Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'ChatGPT': 'data/01-interim/cleaned_chat.csv', \n",
    "    'Gemini': 'data/01-interim/cleaned_gemini.csv', \n",
    "    'Claude': 'data/01-interim/cleaned_claude.csv'}\n",
    "df_list = []\n",
    "\n",
    "for app, file in files.items():\n",
    "    temp_df = pd.read_csv(file)\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# concatenate three datasets\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# drop null review and score data\n",
    "clean_df = df.dropna(subset=['content', 'score'])\n",
    "# replace other null data\n",
    "clean_df['like'] = clean_df['like'].fillna(0)\n",
    "clean_df['version'] = clean_df['version'].fillna('unknown')\n",
    "# drop too short contents that are very likely to be non-substantial\n",
    "clean_df = clean_df[clean_df['content'].str.len() > 2].reset_index(drop=True)\n",
    "\n",
    "# standardize the data\n",
    "clean_df['date'] = pd.to_datetime(clean_df['date'])\n",
    "clean_df['content'] = clean_df['content'].str.replace('\\n', ' ', regex=False)\n",
    "clean_df['content'] = clean_df['content'].str.lower()\n",
    "\n",
    "def translate_to_english(row):\n",
    "    text = row['content']\n",
    "    \n",
    "    if len(str(text)) <= 3:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang == 'en':\n",
    "            return text\n",
    "        translated_text = translate(text, 'en')\n",
    "        return translated_text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "clean_df['content'] = clean_df.progress_apply(translate_to_english, axis=1)\n",
    "# drop emoji and normalize exaggerated words (e.g. sooooo gooood -> so good)\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\.\\!\\?']\", '', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    return \" \".join(text.split())\n",
    "clean_df['content'] = clean_df['content'].apply(normalize_text)\n",
    "clean_df['content'] = clean_df['content'].replace('', np.nan)\n",
    "\n",
    "# drop null review and score data\n",
    "clean_df = clean_df.dropna(subset=['content', 'score'])\n",
    "# drop too short contents that are very likely to be non-substantial\n",
    "clean_df = clean_df[clean_df['content'].str.len() > 2].reset_index(drop=True)\n",
    "\n",
    "file_path = 'data/02-processed/cleaned_reviews.csv'\n",
    "clean_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics \n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "> The project uses publicly available app reviews that were written voluntarily by the users. The reviewers did not provide informed consent for their review to be used in research, but the data was collected publically from the Google Play store with no private information being collected. In those places you can only see a user’s username. The people who got their reviews scrapped may not have posted it if they knew that it would be used as data for our research question, so we will be analyzing the text of the review without any private information.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "> We recognize that a user who leaves reviews may not be the same as a person who doesn’t leave reviews. People who leave reviews are likely to report extreme positive or negative experiences, so their language may not reflect the population.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> User data such as usernames will be cleaned to not show any personal information.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "> We are not collecting protected attributes. That information is not displayed and even if it was available it was not going to be used because it is not a part of our analysis.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "> The security is not too advanced but the data will be stored on a private GitHub repo with our group members and the TAs being able to access it.\n",
    "\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "> The analysis only focuses on users who choose to leave written reviews, which may exclude the perspective of users who have downloaded and use the app but haven’t left a review. Also during our meetings, we have decided to scrape only Google Play reviews and not App Store anymore due to the App Store being more difficult, meaning Apple users may be excluded.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to miti\n",
    " - [ ] gate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "> The text of the reviews may be biased by cultural and linguistic differences in how people talk. Even if we take only English reviews, there are many different places that speak English differently in how they speak and text. For example, some cultures may use “thank you” more often out of habit rather than treating the AI like a human.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "> We are not analyzing data with PPI. It is just the raw text of their reviews. This means that no private information is being used, and therefore the analysis will be in privacy.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "> The sentiment analysis analyzes the reviews in English, so non-English reviews will have to be translated. Translation is not perfect and meanings and intent can be lost during translation.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "> Our analysis uses a model from GitHub to classify text as \"anthropomorphic\" or “instrumental”. To ensure explainability, we have reviewed the repository’s documentation and tried to understand how it works. We can justify the model’s decision making by looking at some of the reviews and comparing the score to what we would expect to see if the logic aligns with how we think.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "> The way the score of the user reviews is inferred from the language algorithm and is not a definitive measure of a users’ belief. The analysis is correlational and limited to English reviews from the platform and the AI app. Therefore, the results may not generalize to all people who use AI.\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "> The app review data was scrapped and stored in a CSV file for analysis. This means it is a static dataset and does not have a deployable model that doesn't have to be rolled back and we are not monitoring the model. Because of this, the risk of unintended use is minimal. We are just analyzing the reviews for our project. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Team Expectation 1*: Do not show up late for meetings\n",
    "* *Team Expectation 2*: Complete your section of the work by the deadline of the week\n",
    "* *Team Expecation 3*: If you cannot make it or will be very late, give a heads up\n",
    "* *Team Expecation 4*: Be respectful to each other\n",
    "* *Team Expecation 5*: Attend the team meeting every week\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "| Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/2  |  3 PM | Determine the RQ, do background research on the topic  | Research potential tasets to use; work on research proposal | \n",
    "| 2/4  |  N/A |  Project Proposal | N/A | \n",
    "| 2/9  | 3 PM  | Import and wrangling data  | Discuss data analysis plan   |\n",
    "| 2/16  | 3 PM  | Finalized data wrangling | Discuss and analyze the data; address TA feedback on the project proposal   |\n",
    "| 2/18  | N/A  | Data Checkpoint | N/A |\n",
    "| 2/23  | 3 PM  | N/A | Address feedback from TA and start talking about analysis |\n",
    "| 3/2  | 3 PM  | Complete Analysis | Discuss/draft reselts, discussion, conclusion |\n",
    "| 3/4  | N/A  | EDA Checkpoint | N/A |\n",
    "| 3/9  | 3 PM  | Edit full project | Refine final project |\n",
    "| 3/18  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
